{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "mnist_train = MNIST('mnist', train=True, download=True, transform=ToTensor())\n",
    "mnist_test = MNIST('mnist', train=False, download=True, transform=ToTensor())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#### Unnecassary???####\n",
    "# Determine the mean and standard deviation\n",
    "# mnist_train_pixels = np.concatenate([np.array(x) for x, y in mnist_train], axis=0)\n",
    "# mnist_test_pixels = np.concatenate([np.array(x) for x, y in mnist_test], axis=0)\n",
    "#\n",
    "# mean_train = np.mean(mnist_train_pixels)\n",
    "# mean_test = np.mean(mnist_test_pixels)\n",
    "# print(mean_train, mean_test)\n",
    "#\n",
    "# std_train = np.std(mnist_train_pixels)\n",
    "# std_test = np.std(mnist_test_pixels)\n",
    "# print(std_train, std_test)\n",
    "#\n",
    "# transform_train = transforms.Compose([transforms.ToTensor(),\n",
    "#                                       Normalize(mean_train, std_train)])\n",
    "# transform_test = transforms.Compose([transforms.ToTensor(),\n",
    "#                                         Normalize(mean_test, std_test)])\n",
    "#\n",
    "# mnist_train = MNIST('mnist', train=True, download=True, transform=transform_train)\n",
    "# mnist_test = MNIST('mnist', train=False, download=True, transform=transform_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Split the data into training set (70%) and testing set (30%)\n",
    "mnist_train_size = int(0.7 * len(mnist_train))\n",
    "mnist_test_size = len(mnist_train) - mnist_train_size\n",
    "mnist_train_data, mnist_test_data = torch.utils.data.random_split(mnist_train, [mnist_train_size, mnist_test_size])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(mnist_train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test_data, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define the 2-layer MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, reg_strength):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.reg_strength = reg_strength\n",
    "        # Using 2 hidden layers\n",
    "        # self.fc1 = nn.Linear(input_size * input_size, hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # Using 3 hidden layers\n",
    "        # self.fc1 = nn.Linear(input_size * input_size, hidden_size)\n",
    "        # self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "        # Using 4 hidden layers\n",
    "        self.fc1 = nn.Linear(input_size * input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size * self.input_size)\n",
    "        # Using ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        # Using Tanh\n",
    "        # x = torch.tanh(self.fc1(x))\n",
    "        # x = torch.tanh(self.fc2(x))\n",
    "        # x = torch.tanh(self.fc3(x))\n",
    "        # x = torch.tanh(self.fc4(x))\n",
    "        # Using Sigmoid\n",
    "        # x = torch.sigmoid(self.fc1(x))\n",
    "        # x = torch.sigmoid(self.fc2(x))\n",
    "        # x = torch.sigmoid(self.fc3(x))\n",
    "        # x = torch.sigmoid(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param)\n",
    "        return self.reg_strength * l2_loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP(28, 100, 10, 0.0002)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "# Using CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Using MSELoss\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Add regularization to the loss function\n",
    "def reularized_loss(output, target):\n",
    "    loss = criterion(output, target)\n",
    "    reg_loss = mlp.l2_regularization_loss()\n",
    "    return loss + reg_loss\n",
    "\n",
    "# Using SGD\n",
    "# optimizer = optim.SGD(mlp.parameters(), lr=0.0002, momentum=0.9)\n",
    "\n",
    "# Using Adam\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.0002)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 855.1603162661195\n",
      "Epoch 2 loss: 406.5071855150163\n",
      "Epoch 3 loss: 299.40786983817816\n",
      "Epoch 4 loss: 233.27788413502276\n",
      "Epoch 5 loss: 194.08659490942955\n",
      "Epoch 6 loss: 165.03183232992887\n",
      "Epoch 7 loss: 144.65966725256294\n",
      "Epoch 8 loss: 128.81449887622148\n",
      "Epoch 9 loss: 113.39699163008481\n",
      "Epoch 10 loss: 101.54378873482347\n",
      "Epoch 11 loss: 91.49456492997706\n",
      "Epoch 12 loss: 80.060516923666\n",
      "Epoch 13 loss: 73.87208669818938\n",
      "Epoch 14 loss: 63.629541067406535\n",
      "Epoch 15 loss: 59.91077868267894\n",
      "Epoch 16 loss: 54.51254006661475\n",
      "Epoch 17 loss: 51.37781218905002\n",
      "Epoch 18 loss: 44.55264545790851\n",
      "Epoch 19 loss: 42.29377803392708\n",
      "Epoch 20 loss: 37.97591426130384\n",
      "Epoch 21 loss: 35.977053481154144\n",
      "Epoch 22 loss: 32.51917611062527\n",
      "Epoch 23 loss: 32.215857402421534\n",
      "Epoch 24 loss: 32.41206343751401\n",
      "Epoch 25 loss: 25.910215868614614\n",
      "Epoch 26 loss: 30.19633192103356\n",
      "Epoch 27 loss: 26.58585260435939\n",
      "Epoch 28 loss: 25.48805855680257\n",
      "Epoch 29 loss: 24.180408499203622\n",
      "Epoch 30 loss: 25.30983182694763\n",
      "Epoch 31 loss: 24.499968642368913\n",
      "Epoch 32 loss: 22.441010118462145\n",
      "Epoch 33 loss: 27.071668654680252\n",
      "Epoch 34 loss: 22.372423607856035\n",
      "Epoch 35 loss: 22.071181915700436\n",
      "Epoch 36 loss: 21.174143540672958\n",
      "Epoch 37 loss: 21.304931324906647\n",
      "Epoch 38 loss: 21.61866363044828\n",
      "Epoch 39 loss: 21.40593522321433\n",
      "Epoch 40 loss: 19.824687307700515\n",
      "Epoch 41 loss: 23.273653188720345\n",
      "Epoch 42 loss: 21.14271715003997\n",
      "Epoch 43 loss: 18.638729698956013\n",
      "Epoch 44 loss: 23.26521616615355\n",
      "Epoch 45 loss: 17.16623049043119\n",
      "Epoch 46 loss: 23.97728890646249\n",
      "Epoch 47 loss: 17.264946101233363\n",
      "Epoch 48 loss: 20.595587965101004\n",
      "Epoch 49 loss: 19.14582741074264\n",
      "Epoch 50 loss: 20.277999963611364\n",
      "Total training time: 417.2173869609833\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 50 epochs\n",
    "\n",
    "# (Record start time)\n",
    "\n",
    "training_start_time = time.time()\n",
    "losses = []\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = mlp(data)\n",
    "        loss = reularized_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'Epoch {epoch + 1} loss: {epoch_loss}')\n",
    "\n",
    "# (Record end time)\n",
    "training_end_time = time.time()\n",
    "\n",
    "# (Calculate total time)\n",
    "training_total_time = training_end_time - training_start_time\n",
    "\n",
    "# (Report total time)\n",
    "print(f'Total training time: {training_total_time}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 97.55%\n",
      "Total evaluation time on test set: 1.7249670028686523\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the testing set\n",
    "\n",
    "# (Record start time)\n",
    "eval_test_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in test_loader:\n",
    "        output = mlp(data)\n",
    "        prediction = output.argmax(dim=1)\n",
    "        correct += (prediction == target).sum().item()\n",
    "        total += target.shape[0]\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy on test set: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# (Record end time)\n",
    "eval_test_end_time = time.time()\n",
    "\n",
    "# (Calculate total time)\n",
    "eval_test_total_time = eval_test_end_time - eval_test_start_time\n",
    "\n",
    "# (Report total time)\n",
    "print(f'Total evaluation time on test set: {eval_test_total_time}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results with ReLU:\n",
    "    Accuracy on test set: 95%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 99.98%\n",
      "Total evaluation time on train set: 3.721993923187256\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "\n",
    "# (Record start time)\n",
    "eval_train_start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data, target in train_loader:\n",
    "        output = mlp(data)\n",
    "        prediction = output.argmax(dim=1)\n",
    "        correct += (prediction == target).sum().item()\n",
    "        total += target.shape[0]\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy on train set: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# (Record end time)\n",
    "eval_train_end_time = time.time()\n",
    "\n",
    "# (Calculate total time)\n",
    "eval_train_total_time = eval_train_end_time - eval_train_start_time\n",
    "\n",
    "# (Report total time)\n",
    "print(f'Total evaluation time on train set: {eval_train_total_time}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
